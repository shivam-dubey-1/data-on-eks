"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[341],{5295:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>d,toc:()=>u});var s=t(4848),o=t(8453),i=(t(1470),t(9365),t(2450)),a=t(1432);const r='apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: s3-mount-script\n  namespace: spark-team-a\ndata:\n  monitor_s3_mount.sh: |\n    #!/bin/bash\n\n    set -e  # Exit immediately if a command exits with a non-zero status\n\n    # ENVIRONMENT VARIABLES\n    LOG_FILE="/var/log/s3-mount.log"\n    S3_BUCKET_NAME="<S3_BUCKET_NAME>"  # Replace with your S3 Bucket Name before applying to EKS cluster\n    MOUNT_POINT="/mnt/s3"\n    CACHE_DIR="/tmp"\n    MOUNT_S3_BIN="/usr/bin/mount-s3"\n    MOUNT_S3_URL="https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm"\n\n    # Function to install mount-s3\n    install_mount_s3() {\n      echo "$(date): Installing mount-s3" | tee -a $LOG_FILE\n      yum update -y | tee -a $LOG_FILE\n      yum install -y wget util-linux | tee -a $LOG_FILE\n      wget $MOUNT_S3_URL -O /tmp/mount-s3.rpm | tee -a $LOG_FILE\n      yum install -y /tmp/mount-s3.rpm | tee -a $LOG_FILE\n    }\n\n    # Function to mount S3 bucket\n    mount_s3_bucket() {\n      echo "$(date): Mounting S3 bucket: $S3_BUCKET_NAME to $MOUNT_POINT" | tee -a $LOG_FILE\n      $MOUNT_S3_BIN --metadata-ttl indefinite --allow-other --cache $CACHE_DIR $S3_BUCKET_NAME $MOUNT_POINT | tee -a $LOG_FILE\n      if [ $? -ne 0 ]; then\n        echo "$(date): Failed to mount S3 bucket: $S3_BUCKET_NAME" | tee -a $LOG_FILE\n        exit 1\n      fi\n    }\n\n    # Ensure the mount point directory exists\n    ensure_mount_point() {\n      if [ ! -d $MOUNT_POINT ]; then\n        echo "$(date): Creating mount point directory: $MOUNT_POINT" | tee -a $LOG_FILE\n        mkdir -p $MOUNT_POINT\n      fi\n    }\n\n    # Install mount-s3\n    install_mount_s3\n\n    # Continuous monitoring and remounting loop\n    while true; do\n      echo "$(date): Checking if S3 bucket is mounted" | tee -a $LOG_FILE\n      ensure_mount_point\n      if mount | grep $MOUNT_POINT > /dev/null; then\n        echo "$(date): S3 bucket is already mounted" | tee -a $LOG_FILE\n        if ! ls $MOUNT_POINT > /dev/null 2>&1; then\n          echo "$(date): Transport endpoint is not connected, remounting S3 bucket" | tee -a $LOG_FILE\n          fusermount -u $MOUNT_POINT || echo "$(date): Failed to unmount S3 bucket" | tee -a $LOG_FILE\n          rm -rf $MOUNT_POINT || echo "$(date): Failed to remove mount point directory" | tee -a $LOG_FILE\n          ensure_mount_point\n          mount_s3_bucket\n        fi\n      else\n        echo "$(date): S3 bucket is not mounted, mounting now" | tee -a $LOG_FILE\n        mount_s3_bucket\n      fi\n      sleep 60  # Check every 60 seconds\n    done\n\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: s3-mount-daemonset\n  namespace: spark-team-a\nspec:\n  selector:\n    matchLabels:\n      name: s3-mount-daemonset\n  template:\n    metadata:\n      labels:\n        name: s3-mount-daemonset\n    spec:\n      hostPID: true\n      hostIPC: true\n      hostNetwork: true\n      volumes:\n      - name: script\n        configMap:\n          name: s3-mount-script\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      restartPolicy: Always\n      containers:\n      - name: s3-mount\n        image: amazonlinux:2\n        volumeMounts:\n        - name: script\n          mountPath: /config\n        - name: host-root\n          mountPath: /host\n          mountPropagation: Bidirectional\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        - |\n          set -e\n          echo "Starting s3-mount"\n          yum install -y util-linux\n          echo "Copying script to /usr/bin"\n          cp /config/monitor_s3_mount.sh /host/usr/bin/monitor_s3_mount.sh\n          chmod +x /host/usr/bin/monitor_s3_mount.sh\n          echo "Verifying the copied script"\n          ls -lha /host/usr/bin/monitor_s3_mount.sh\n          echo "Running the script in Host space"\n          nsenter --target 1 --mount --uts --ipc --net --pid ./usr/bin/monitor_s3_mount.sh\n          echo "Done"\n',l={sidebar_position:3,sidebar_label:"Mounpoint-S3 for Spark Workloads"},c="Mountpoint-S3 for Spark Workloads",d={id:"resources/mountpoint-s3-for-spark",title:"Mountpoint-S3 for Spark Workloads",description:"When working with the SparkApplication Custom Resource Definition (CRD) managed by the SparkOperator, handling multiple dependency JAR files can become a significant challenge. Traditionally, these JAR files are bundled within the container image, leading to several inefficiencies:",source:"@site/docs/resources/mountpoint-s3-for-spark.md",sourceDirName:"resources",slug:"/resources/mountpoint-s3-for-spark",permalink:"/data-on-eks/docs/resources/mountpoint-s3-for-spark",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/resources/mountpoint-s3-for-spark.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,sidebar_label:"Mounpoint-S3 for Spark Workloads"},sidebar:"resources",previous:{title:"Mounpoint-S3 on EKS",permalink:"/data-on-eks/docs/resources/mountpoint-s3"}},h={},u=[{value:"What is Mountpoint-S3?",id:"what-is-mountpoint-s3",level:2},{value:"Using Mountpoint-S3 with EKS",id:"using-mountpoint-s3-with-eks",level:2},{value:"Resource Allocation",id:"resource-allocation",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Approach 1: Deploy Mountpoint-S3 on EKS at <em>Pod level</em>",id:"approach-1-deploy-mountpoint-s3-on-eks-at-pod-level",level:2},{value:"Approach 2:  Deploy Mountpoint-S3 on EKS at <em>Node level</em>",id:"approach-2--deploy-mountpoint-s3-on-eks-at-node-level",level:2},{value:"Approach 2.1: Using USERDATA",id:"approach-21-using-userdata",level:3},{value:"Userdata:",id:"userdata",level:4},{value:"Approach 2.2: Using DaemonSet",id:"approach-22-using-daemonset",level:3},{value:"Executing Spark Job",id:"executing-spark-job",level:2},{value:"Verification",id:"verification",level:2},{value:"Conclusion",id:"conclusion",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"mountpoint-s3-for-spark-workloads",children:"Mountpoint-S3 for Spark Workloads"}),"\n",(0,s.jsxs)(n.p,{children:["When working with the ",(0,s.jsx)(n.a,{href:"https://www.kubeflow.org/docs/components/spark-operator/user-guide/using-sparkapplication/",children:"SparkApplication"})," Custom Resource Definition (CRD) managed by the ",(0,s.jsx)(n.a,{href:"https://github.com/kubeflow/spark-operator",children:"SparkOperator"}),", handling multiple dependency JAR files can become a significant challenge. Traditionally, these JAR files are bundled within the container image, leading to several inefficiencies:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Increased Build Time:"})," Downloading and adding JAR files during the build process significantly inflates the build time of the container image."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Larger Image Size:"})," Including JAR files directly in the container image increases its size, resulting in longer download times when pulling the image to execute jobs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Frequent Rebuilds:"})," Any updates or additions to the dependency JAR files necessitate rebuilding and redeploying the container image, further increasing operational overhead."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/features/mountpoint/",children:"Mountpoint for Amazon S3"})," offers an effective solution to these challenges. As an open-source file client, Mountpoint-S3 allows you to mount an S3 bucket on your compute instance, making it accessible as a local virtual file system. It automatically translates local file system API calls into REST API calls on S3 objects, providing seamless integration with Spark jobs."]}),"\n",(0,s.jsx)(n.h2,{id:"what-is-mountpoint-s3",children:"What is Mountpoint-S3?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/awslabs/mountpoint-s3",children:"Mountpoint-S3"})," is an open-source file client developed by AWS that translates file operations into S3 API calls, enabling your applications to interact with ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/",children:"Amazon S3"})," buckets as if they were local disks. ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/features/mountpoint/",children:"Mountpoint for Amazon S3"})," is optimized for applications that need high read throughput to large objects, potentially from many clients at once, and to write new objects sequentially from a single client at a time. It offers significant performance gains compared to traditional S3 access methods, making it ideal for data-intensive workloads or AI/ML training."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/features/mountpoint/",children:"Mountpoint for Amazon S3"})," is optimized for high-throughput performance, largely due to its foundation on the ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html",children:"AWS Common Runtime (CRT)"})," library. The CRT library is a collection of libraries and modules designed to deliver high performance and low resource usage, specifically tailored for AWS services. Key features of the CRT library that enable high-throughput performance include:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient I/O Management:"})," The CRT library is optimized for non-blocking I/O operations, reducing latency and maximizing the utilization of network bandwidth."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lightweight and Modular Design:"})," The library is designed to be lightweight, with minimal overhead, allowing it to perform efficiently even under high load. Its modular architecture ensures that only the necessary components are loaded, further enhancing performance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Memory Management:"})," CRT employs advanced memory management techniques to minimize memory usage and reduce garbage collection overhead, leading to faster data processing and reduced latency."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimized Network Protocols:"})," The CRT library includes optimized implementations of network protocols, such as HTTP/2, that are specifically tuned for AWS environments. These optimizations ensure rapid data transfer between S3 and your compute instances, which is critical for large-scale Spark workloads."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"using-mountpoint-s3-with-eks",children:"Using Mountpoint-S3 with EKS"}),"\n",(0,s.jsxs)(n.p,{children:["For Spark workloads, we'll specifically focus on ",(0,s.jsx)(n.strong,{children:"loading external JARs located in S3 for Spark Applications"}),". We\u2019ll examine two primary deployment strategies for Mountpoint-S3;"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Leveraging the ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html",children:"EKS Managed Addon CSI driver"})," with Persistent Volumes (PV) and Persistent Volume Claims (PVC)"]}),"\n",(0,s.jsxs)(n.li,{children:["Deploying Mountpoint-S3 at the Node level using either ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html",children:"USERDATA"})," scripts or DaemonSets."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The first approach is considered mounting at a Pod level because the PV created is available to individual pods. The second Approach is considered mounting at a Node level because the S3 is mounted on the host itself. Each approach is discussed in detail below, highlighting their respective strengths and considerations to help you determine the most effective solution for your specific use case."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Metric"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Pod Level"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Node Level"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Access Control"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Provides fine-grained access control through service roles and RBAC, limiting PVC access to specific Pods. This is not possible with host-level mounts, where the mounted S3 bucket is accessible to all Pods on the Node."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Simplifies configuration but lacks the granular control offered by Pod-level mounting."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Scalabbility and Overhead"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Involves managing individual PVCs, which can increase overhead in large-scale environments."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Reduces configuration complexity but provides less isolation between Pods."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Performance Considerations"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Offers predictable and isolated performance for individual Pods."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"May lead to contention if multiple Pods on the same Node access the same S3 bucket."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Flexibility and Use Cases"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Best suited for use cases where different Pods require access to different datasets or where strict security and compliance controls are necessary."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Ideal for environments where all Pods on a Node can share the same dataset, such as when running batch processing jobs or Spark jobs that require common dependencies."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"resource-allocation",children:"Resource Allocation"}),"\n",(0,s.jsx)(n.p,{children:"Before being able to implement the Mountpoint-s3 solution provided, AWS cloud resources need to be allocated. To do deploy the Terraform stack following the instructions below. After allocating the resources and setting up the EKS environment, you can explore the two different approaches of utilizing Mountpoint-S3 in detail."}),"\n",(0,s.jsxs)(i.A,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Deploy Solution Resources"})}),children:[(0,s.jsxs)(n.p,{children:["In this ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator",children:"example"}),", you will provision the following resources required to run Spark Jobs with open source Spark Operator."]}),(0,s.jsx)(n.p,{children:"This example deploys an EKS Cluster running the Spark K8s Operator into a new VPC."}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Creates a new sample VPC, 2 Private Subnets and 2 Public Subnets"}),"\n",(0,s.jsx)(n.li,{children:"Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets"}),"\n",(0,s.jsx)(n.li,{children:"Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with core managed Node group, on-demand Node group and Spot Node group for Spark workloads."}),"\n",(0,s.jsx)(n.li,{children:"Deploys Metrics server, Cluster Autoscaler, Spark-k8s-operator, Apache Yunikorn, Karpenter, Grafana, AMP and Prometheus server."}),"\n"]}),(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,s.jsx)(n.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,s.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,s.jsx)(n.p,{children:"Clone the repository."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\ncd data-on-eks\nexport DOEKS_HOME=$(pwd)\n"})}),(0,s.jsxs)(n.p,{children:["If DOEKS_HOME is ever unset, you can always set it manually using ",(0,s.jsx)(n.code,{children:"export DATA_ON_EKS=$(pwd)"})," from your data-on-eks directory."]}),(0,s.jsxs)(n.p,{children:["Navigate into one of the example directories and run ",(0,s.jsx)(n.code,{children:"install.sh"})," script."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator\nchmod +x install.sh\n./install.sh\n"})}),(0,s.jsx)(n.p,{children:"Now create an S3_BUCKET variable that holds the name of the bucket created\nduring the install. This bucket will be used in later examples to store output\ndata. If S3_BUCKET is ever unset, you can run the following commands again."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server)\necho $S3_BUCKET\n"})})]}),"\n",(0,s.jsxs)(n.h2,{id:"approach-1-deploy-mountpoint-s3-on-eks-at-pod-level",children:["Approach 1: Deploy Mountpoint-S3 on EKS at ",(0,s.jsx)(n.em,{children:"Pod level"})]}),"\n",(0,s.jsxs)(n.p,{children:["Deploying Mountpoint-S3 at the Pod level involves using the ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html",children:"EKS Managed Addon CSI driver"})," with Persistent Volumes (PV) and Persistent Volume Claims (PVC) to mount an S3 bucket directly within a Pod. This method allows for fine-grained control over which Pods can access specific S3 buckets, ensuring that only the necessary workloads have access to the required data."]}),"\n",(0,s.jsxs)(n.p,{children:["Once ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/mountpoint-s3",children:"Mountpoint-S3"})," is enabled and the PV is created, the S3 bucket becomes a cluster-level resource, allowing any Pod to request access by creating a PVC that references the PV. To achieve fine-grained control over which Pods can access specific PVCs, you can use service roles within namespaces. By assigning specific service accounts to Pods and defining Role-Based Access Control (RBAC) policies, you can limit which Pods can bind to certain PVCs. This ensures that only authorized Pods can mount the S3 bucket, providing tighter security and access control compared to a host-level mount, where the hostPath is accessible to all Pods on the Node."]}),"\n",(0,s.jsxs)(n.p,{children:["Using this approach can also be simplified using the ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html",children:"EKS Managed Addon CSI driver"}),". However, this does not support taints/tolerations and therefore cannot be used with GPUs. Additionally, because the Pods are not sharing the mount and therefore not sharing the cache it would lead to more S3 API calls."]}),"\n",(0,s.jsxs)(n.p,{children:["For more information on how to deploy this approach refer to the ",(0,s.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/resources/mountpoint-s3",children:"deployment instructions"})]}),"\n",(0,s.jsxs)(n.h2,{id:"approach-2--deploy-mountpoint-s3-on-eks-at-node-level",children:["Approach 2:  Deploy Mountpoint-S3 on EKS at ",(0,s.jsx)(n.em,{children:"Node level"})]}),"\n",(0,s.jsxs)(n.p,{children:["Mounting a S3 Bucket at a Node level can streamline the management of dependency JAR files for SparkApplications by  reducing build times and speeding up deployment. It can be implemented using either ",(0,s.jsx)(n.strong,{children:"USERDATA"})," or ",(0,s.jsx)(n.strong,{children:"DaemonSet."})," USERDATA is the preferred method for implementing ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/mountpoint-s3",children:"Mountpoint-S3"}),". However, if you have static Nodes in your EKS cluster that you cannot bring down, the DaemonSet approach provides an alternative. Make sure to understand all of the security mechanisms that need to be enabled in order to utilize the DaemonSet approach before implementing it."]}),"\n",(0,s.jsx)(n.h3,{id:"approach-21-using-userdata",children:"Approach 2.1: Using USERDATA"}),"\n",(0,s.jsxs)(n.p,{children:["This approach is recommended for new clusters or where auto-scaling is customized to run workloads as the user-data script is run when a Node is initialized. Using the below script, the Node can be updated to have the S3 bucket mounted upon initialization in the EKS cluster that hosts the Pods. The below script outlines downloading, installing, and running the Mountpoint S3 package. There are a couple of arguments that set for this application that can be altered depending on the use case. More information about this arguments can be found ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#caching-configuration",children:"here"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"metadata-ttl: this is set to indefinite because the jar files are meant to be used as read only and will not change."}),"\n",(0,s.jsx)(n.li,{children:"allow-others: this is set so that the Node can have access to the mounted volume when using SSM"}),"\n",(0,s.jsx)(n.li,{children:"cache: this is set to enable caching and limit the S3 API calls that need to be made by storing the files in cache for consecutive re-reads."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In addition to these arguments that are set by this example, there are also a number of other options for additional logging and debugging. This information  can be found ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/mountpoint-s3/blob/main/doc/LOGGING.md",children:"here"})]}),"\n",(0,s.jsxs)(n.p,{children:["When autoscaling with ",(0,s.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," this method allows for more flexibility and performance. For example when configuring Karpenter in the terraform code, the user data for different types of Nodes can be unique with different buckets depending on the workload so when Pods are scheduled and need a certain set of dependencies, Taints and Tolerations will allow Karpenter to allocate the specific instance type with the unique user data to ensure the correct bucket with the dependent files is mounted on the Node so that Pods can access is."]}),"\n",(0,s.jsx)(n.h4,{id:"userdata",children:"Userdata:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"#!/bin/bash\nyum update -y\nyum install -y wget\nwget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm\nyum install -y mount-s3.rpm mkdir -p /mnt/s3\n/opt/aws/mountpoint-s3/bin/mount-s3 --metadata-ttl indefinite --allow-other --cache /tmp <S3_BUCKET_NAME> /mnt/s3\n"})}),"\n",(0,s.jsx)(n.h3,{id:"approach-22-using-daemonset",children:"Approach 2.2: Using DaemonSet"}),"\n",(0,s.jsx)(n.p,{children:"This approach is recommended for existing clusters. This approach is made up of 2 resources, a ConfigMap with a script that maintains the S3 Mount Point package onto the Node and a DaemonSet that runs a Pod on every Node in the cluster which will execute the script on the Node."}),"\n",(0,s.jsx)(n.p,{children:"The ConfigMap script will run a loop to check the mountPoint every 60 seconds and remount it if there are any issues. There are multiple environment variables that can be altered for the mount location, cache location, S3 bucket name, log file location, and the URL of the package installation and the location of the of the installed package. these variables can be left as default as only the S3 bucket name is required to run."}),"\n",(0,s.jsxs)(n.p,{children:["The DaemonSet Pods will copy the script onto the Node, alter the permissions to allow execution, and then finally run the script. The Pod installs util-linux in order to have access to ",(0,s.jsx)(n.a,{href:"https://man7.org/linux/man-pages/man1/nsenter.1.html",children:"nsenter"}),", which allows the Pod execute the script in the Node space which allows the S3 Bucket to be mounted on to the Node and not the Pod."]}),"\n",(0,s.jsx)(n.admonition,{type:"danger",children:(0,s.jsx)(n.p,{children:"The DaemonSet Pod requires the securityContext to be privileged as well as hostPID, hostIPC, and hostNetwork to be set to true.\nreview below why these are required to be configured for this solution and their security implications."})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["securityContext: privileged","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Purpose: privileged mode gives the container full access to all host resources, similar to root access on the host."}),"\n",(0,s.jsx)(n.li,{children:"To install software packages, configure the system, and mount the S3 bucket onto the host, your container will likely need elevated permissions. Without privileged mode, the container might not have sufficient permissions to perform these actions on the host filesystem and network interfaces."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["hostPID","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Purpose: nsenter allows you to enter various namespaces, including the PID namespace of the host."}),"\n",(0,s.jsxs)(n.li,{children:["When using nsenter to enter the host\u2019s PID namespace, the container needs access to the host\u2019s PID namespace. Thus, enabling ",(0,s.jsx)(n.code,{children:"hostPID: true"})," is necessary to interact with processes on the host, which is crucial for operations like installing packages or running commands that require host-level process visibility like mountpoint-s3."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["hostIPC","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Purpose: hostIPC enables your container to share the host\u2019s inter-process communication namespace, which includes shared memory."}),"\n",(0,s.jsxs)(n.li,{children:["If nsenter commands or the script to run involves shared memory or other IPC mechanisms on the host, ",(0,s.jsx)(n.code,{children:"hostIPC: true"})," will be necessary. While it\u2019s less common than hostPID, it\u2019s often enabled alongside it when nsenter is involved, especially if the script needs to interact with host processes that rely on IPC."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["hostNetwork","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Purpose: hostNetwork allows the container to use the host\u2019s network namespace, giving the container access to the host\u2019s IP address and network interfaces."}),"\n",(0,s.jsx)(n.li,{children:"During the installation process, the script will likely need to download packages from the internet (e.g., from repositories hosting the mountpoint-s3 package). By enabling hostNetwork, you ensure that the download processes have direct access to the host\u2019s network interface, avoiding issues with network isolation."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["This sample code uses the ",(0,s.jsx)(n.code,{children:"spark-team-a"})," namespace to run the job and host the DaemonSet. This is primarily because the Terraform stack already sets up ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html",children:"IRSA"})," for this namespace and allows the service account to access any S3 bucket.\nWhen using in production make sure create your own separate namespace, service account, and IAM role that follows the policy of least-privilege permissions and follows ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",children:"IAM role best practice"})]})}),"\n",(0,s.jsx)("TO-DO",{children:" expand on why hostPID, hostIPC and network. Also give a disclaimer on the the namespace for buckets and the IRSA for the name spark"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:" To view the DaemonSet, Click to toggle content!"}),(0,s.jsx)(a.A,{language:"yaml",children:r})]}),"\n",(0,s.jsx)(n.h2,{id:"executing-spark-job",children:"Executing Spark Job"}),"\n",(0,s.jsx)(n.p,{children:"Lets\u2019 test the scenario using Approach-2 with DaemonSet"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Deploy ",(0,s.jsx)(n.a,{href:"#resource-allocation",children:"Spark Operator Resources"})]}),"\n",(0,s.jsxs)(n.li,{children:["Prepare the S3 Bucket","\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/mountpoint-s3-spark/"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"chmod +x copy-jars-to-s3.sh"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"./copy-jars-to-s3.sh"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Set-up Kubeconfig","\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"aws eks update-kubeconfig --name spark-operator-doeks"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Apply DaemonSet","\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"kubectl apply -f mountpoint-s3-daemonset.yaml "})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Apply Spark Job sample","\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"kubectl apply -f mountpoint-s3-spark-job.yaml "})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["View Job Running","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["There are a couple different resources of which we can view logs of as this SparkApplication CRD is running. Each of these logs should be in a separate terminal to view all of the logs simultaneously.","\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"spark operator"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:" kubectl -n spark-operator get pods"})}),"\n",(0,s.jsx)(n.li,{children:"copy the name of the spark operator pod"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:" kubectl -n spark-operator logs -f <POD_NAME>"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"spark-team-a Pods"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"In order to get the logs for the driver and exec Pods for the SparkApplication, we need to first verify that the Pods are running. using wide output we should be able to see the Node that the Pods are running on and using -w we can see the status updates for each of the Pods."}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"kubectl -n spark-team-a get pods -o wide -w"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"driver Pod"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Once the driver Pod is in the running state which will be visible in the previous terminal, we can get the logs for the driver Pod"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"kubectl -n spark-team-a logs -f taxi-trip"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"exec Pod"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Once the exec Pod is in the running state which will be visible in the previous terminal, we can get the logs for the exec Pod"}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"kubectl -n spark-team-a logs -f taxi-trip-exec-1"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"verification",children:"Verification"}),"\n",(0,s.jsx)(n.p,{children:"Once the job is done running you can see in the exec logs that the files are being copied from the local mountpoint-s3 location on the Node to the spark Pod in order to do the processing."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"24/08/13 00:08:46 INFO Utils: Copying /mnt/s3/jars/hadoop-aws-3.3.1.jar to /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/-14716855631723507720806_cache\n24/08/13 00:08:46 INFO Utils: Copying /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/-14716855631723507720806_cache to /opt/spark/work-dir/./hadoop-aws-3.3.1.jar\n24/08/13 00:08:46 INFO Executor: Adding file:/opt/spark/work-dir/./hadoop-aws-3.3.1.jar to class loader\n24/08/13 00:08:46 INFO Executor: Fetching file:/mnt/s3/jars/aws-java-sdk-bundle-1.12.647.jar with timestamp 1723507720806\n24/08/13 00:08:46 INFO Utils: Copying /mnt/s3/jars/aws-java-sdk-bundle-1.12.647.jar to /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/14156613201723507720806_cache\n24/08/13 00:08:47 INFO Utils: Copying /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/14156613201723507720806_cache to /opt/spark/work-dir/./aws-java-sdk-bundle-1.12.647.jar\n"})}),"\n",(0,s.jsx)(n.p,{children:"Additionally, when viewing status of the spark-team-a Pods, you would notice that another Node comes online, this Node is is optimized to the run the SparkApplication and as soon as it comes online the DaemonSet Pod will also spin up and start running on the new Node so that any Pods that are run that new Node will also have access to the S3 Bucket. Using Systems Sessions Manager (SSM), you can connect any of the Nodes and verify the that the mountpoint-s3 package has been downloaded and installed by running:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"mount-s3 --version"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The largest advantage to using the mountpoint-S3 on the Node level for multiple Pods is that the data can be cached to allow other Pods to access the same data without having to make their own API calls. Once the ",(0,s.jsx)(n.em,{children:"karpenter-spark-compute-optimized"})," optimized Node is allocated you can use Sessions Manager (SSM) to connect to the Node and verify that the files will be cached on the Node when the job is run and the volume is mounted. you can see the cache at:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"sudo ls /tmp/mountpoint-cache/"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"By leveraging the CRT library, Mountpoint for Amazon S3 can deliver the high throughput and low latency needed to efficiently manage and access large volumes of data stored in S3. This allows dependency JAR files to be stored and managed externally from the container image, decoupling them from the Spark jobs. Additionally, storing JARs in S3 enables multiple Pods to consume them, leading to cost savings as S3 provides a cost-effective storage solution compared to larger container images. S3 also offers virtually unlimited storage, making it easy to scale and manage dependencies."}),"\n",(0,s.jsx)(n.p,{children:"Mountpoint-S3 offers a versatile and powerful way to integrate S3 storage with EKS for data and AI/ML workloads. Whether you choose to deploy it at the Pod level using PVs and PVCs, or at the Node level using USERDATA or DaemonSets, each approach has its own set of advantages and trade-offs. By understanding these options, you can make informed decisions to optimize your data and AI/ML workflows on EKS."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},9365:(e,n,t)=>{t.d(n,{A:()=>a});t(6540);var s=t(4164);const o={tabItem:"tabItem_Ymn6"};var i=t(4848);function a(e){let{children:n,hidden:t,className:a}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,s.A)(o.tabItem,a),hidden:t,children:n})}},1470:(e,n,t)=>{t.d(n,{A:()=>v});var s=t(6540),o=t(4164),i=t(3104),a=t(6347),r=t(205),l=t(7485),c=t(1682),d=t(679);function h(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:s,default:o}}=e;return{value:n,label:t,attributes:s,default:o}}))}(t);return function(e){const n=(0,c.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const o=(0,a.W6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(i),(0,s.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(o.location.search);n.set(i,e),o.replace({...o.location,search:n.toString()})}),[i,o])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,i=u(e),[a,l]=(0,s.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const s=t.find((e=>e.default))??t[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:i}))),[c,h]=m({queryString:t,groupId:o}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[o,i]=(0,d.Dv)(t);return[o,(0,s.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:o}),b=(()=>{const e=c??f;return p({value:e,tabValues:i})?e:null})();(0,r.A)((()=>{b&&l(b)}),[b]);return{selectedValue:a,selectValue:(0,s.useCallback)((e=>{if(!p({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),g(e)}),[h,g,i]),tabValues:i}}var g=t(2303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(4848);function j(e){let{className:n,block:t,selectedValue:s,selectValue:a,tabValues:r}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),d=e=>{const n=e.currentTarget,t=l.indexOf(n),o=r[t].value;o!==s&&(c(n),a(o))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":t},n),children:r.map((e=>{let{value:n,label:t,attributes:i}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,ref:e=>l.push(e),onKeyDown:h,onClick:d,...i,className:(0,o.A)("tabs__item",b.tabItem,i?.className,{"tabs__item--active":s===n}),children:t??n},n)}))})}function w(e){let{lazy:n,children:t,selectedValue:o}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===o));return e?(0,s.cloneElement)(e,{className:"margin-top--md"}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function y(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,o.A)("tabs-container",b.tabList),children:[(0,x.jsx)(j,{...n,...e}),(0,x.jsx)(w,{...n,...e})]})}function v(e){const n=(0,g.A)();return(0,x.jsx)(y,{...e,children:h(e.children)},String(n))}},2450:(e,n,t)=>{t.d(n,{A:()=>m});var s=t(6540),o=t(5556),i=t.n(o),a=t(4164);const r="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=t(4848);function p(e){let{children:n,header:t}=e;const[o,i]=(0,s.useState)(!1);return(0,u.jsxs)("div",{className:r,children:[(0,u.jsxs)("div",{className:(0,a.A)(l,{[h]:o}),onClick:()=>{i(!o)},children:[t,(0,u.jsx)("span",{className:(0,a.A)(c,{[h]:o}),children:o?"\ud83d\udc47":"\ud83d\udc48"})]}),o&&(0,u.jsx)("div",{className:d,children:n})]})}p.propTypes={children:i().node.isRequired,header:i().node.isRequired};const m=p}}]);